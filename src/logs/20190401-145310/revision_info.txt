arguments: train_tripletloss.py
--------------------
tensorflow version: 1.12.0
--------------------
git hash: b'db8cc08f0b59eb9696f8ad7e64197586f7e947d4'
--------------------
b'diff --git a/src/compare.py b/src/compare.py\nindex bc53cc4..67a3735 100644\n--- a/src/compare.py\n+++ b/src/compare.py\n@@ -1,48 +1,23 @@\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import os\n+os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n+os.environ["CUDA_VISIBLE_DEVICES"] = "1"\n from scipy import misc\n import tensorflow as tf\n import numpy as np\n import sys\n-import os\n import copy\n import argparse\n import facenet\n import align.detect_face\n \n def main(args):\n-\n     images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n     with tf.Graph().as_default():\n-\n         with tf.Session() as sess:\n-      \n             # Load the model\n             facenet.load_model(args.model)\n     \ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\nindex 4556bfa..1e2b497 100644\n--- a/src/decode_msceleb_dataset.py\n+++ b/src/decode_msceleb_dataset.py\n@@ -1,28 +1,3 @@\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\ndiff --git a/src/facenet.py b/src/facenet.py\nindex 0e05676..a2755c3 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -1,28 +1,4 @@\n-"""Functions for building the face recognition network.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-# pylint: disable=missing-docstring\n+#coding=utf-8\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n@@ -42,57 +18,57 @@ import math\n from six import iteritems\n \n def triplet_loss(anchor, positive, negative, alpha):\n-    """Calculate the triplet loss according to the FaceNet paper\n-    \n-    Args:\n-      anchor: the embeddings for the anchor images.\n-      positive: the embeddings for the positive images.\n-      negative: the embeddings for the negative images.\n+\t"""Calculate the triplet loss according to the FaceNet paper\n+\t\n+\tArgs:\n+\t  anchor: the embeddings for the anchor images.\n+\t  positive: the embeddings for the positive images.\n+\t  negative: the embeddings for the negative images.\n   \n-    Returns:\n-      the triplet loss according to the FaceNet paper as a float tensor.\n-    """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n-    return loss\n+\tReturns:\n+\t  the triplet loss according to the FaceNet paper as a float tensor.\n+\t"""\n+\twith tf.variable_scope(\'triplet_loss\'):\n+\t\tpos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n+\t\tneg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n+\t\t\n+\t\tbasic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n+\t\tloss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n+\t  \n+\treturn loss\n   \n def center_loss(features, label, alfa, nrof_classes):\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\n-    """\n-    nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n-    label = tf.reshape(label, [-1])\n-    centers_batch = tf.gather(centers, label)\n-    diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n-    with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n-    return loss, centers\n+\t"""Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n+\t   (http://ydwen.github.io/papers/WenECCV16.pdf)\n+\t"""\n+\tnrof_features = features.get_shape()[1]\n+\tcenters = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n+\t\tinitializer=tf.constant_initializer(0), trainable=False)\n+\tlabel = tf.reshape(label, [-1])\n+\tcenters_batch = tf.gather(centers, label)\n+\tdiff = (1 - alfa) * (centers_batch - features)\n+\tcenters = tf.scatter_sub(centers, label, diff)\n+\twith tf.control_dependencies([centers]):\n+\t\tloss = tf.reduce_mean(tf.square(features - centers_batch))\n+\treturn loss, centers\n \n def get_image_paths_and_labels(dataset):\n-    image_paths_flat = []\n-    labels_flat = []\n-    for i in range(len(dataset)):\n-        image_paths_flat += dataset[i].image_paths\n-        labels_flat += [i] * len(dataset[i].image_paths)\n-    return image_paths_flat, labels_flat\n+\timage_paths_flat = []\n+\tlabels_flat = []\n+\tfor i in range(len(dataset)):\n+\t\timage_paths_flat += dataset[i].image_paths\n+\t\tlabels_flat += [i] * len(dataset[i].image_paths)\n+\treturn image_paths_flat, labels_flat\n \n def shuffle_examples(image_paths, labels):\n-    shuffle_list = list(zip(image_paths, labels))\n-    random.shuffle(shuffle_list)\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n-    return image_paths_shuff, labels_shuff\n+\tshuffle_list = list(zip(image_paths, labels))\n+\trandom.shuffle(shuffle_list)\n+\timage_paths_shuff, labels_shuff = zip(*shuffle_list)\n+\treturn image_paths_shuff, labels_shuff\n \n def random_rotate_image(image):\n-    angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n+\tangle = np.random.uniform(low=-10.0, high=10.0)\n+\treturn misc.imrotate(image, angle, \'bicubic\')\n   \n # 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n RANDOM_ROTATE = 1\n@@ -101,471 +77,494 @@ RANDOM_FLIP = 4\n FIXED_STANDARDIZATION = 8\n FLIP = 16\n def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n-    images_and_labels_list = []\n-    for _ in range(nrof_preprocess_threads):\n-        filenames, label, control = input_queue.dequeue()\n-        images = []\n-        for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            #pylint: disable=no-member\n-            image.set_shape(image_size + (3,))\n-            images.append(image)\n-        images_and_labels_list.append([images, label])\n-\n-    image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n-        shapes=[image_size + (3,), ()], enqueue_many=True,\n-        capacity=4 * nrof_preprocess_threads * 100,\n-        allow_smaller_final_batch=True)\n-    \n-    return image_batch, label_batch\n+\timages_and_labels_list = []\n+\tfor _ in range(nrof_preprocess_threads):\n+\t\tfilenames, label, control = input_queue.dequeue()\n+\t\timages = []\n+\t\tfor filename in tf.unstack(filenames):\n+\t\t\tfile_contents = tf.read_file(filename)\n+\t\t\timage = tf.image.decode_image(file_contents, 3)\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n+\t\t\t\t\t\t\tlambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n+\t\t\t\t\t\t\tlambda:tf.random_crop(image, image_size + (3,)), \n+\t\t\t\t\t\t\tlambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n+\t\t\t\t\t\t\tlambda:tf.image.random_flip_left_right(image),\n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n+\t\t\t\t\t\t\tlambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n+\t\t\t\t\t\t\tlambda:tf.image.per_image_standardization(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], FLIP),\n+\t\t\t\t\t\t\tlambda:tf.image.flip_left_right(image),\n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\t#pylint: disable=no-member\n+\t\t\timage.set_shape(image_size + (3,))\n+\t\t\timages.append(image)\n+\t\timages_and_labels_list.append([images, label])\n+\n+\timage_batch, label_batch = tf.train.batch_join(\n+\t\timages_and_labels_list, batch_size=batch_size_placeholder, \n+\t\tshapes=[image_size + (3,), ()], enqueue_many=True,\n+\t\tcapacity=4 * nrof_preprocess_threads * 100,\n+\t\tallow_smaller_final_batch=True)\n+\t\n+\treturn image_batch, label_batch\n \n def get_control_flag(control, field):\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n+\treturn tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n   \n def _add_loss_summaries(total_loss):\n-    """Add summaries for losses.\n+\t"""Add summaries for losses.\n   \n-    Generates moving average for all losses and associated summaries for\n-    visualizing the performance of the network.\n+\tGenerates moving average for all losses and associated summaries for\n+\tvisualizing the performance of the network.\n   \n-    Args:\n-      total_loss: Total loss from loss().\n-    Returns:\n-      loss_averages_op: op for generating moving averages of losses.\n-    """\n-    # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\n+\tArgs:\n+\t  total_loss: Total loss from loss().\n+\tReturns:\n+\t  loss_averages_op: op for generating moving averages of losses.\n+\t"""\n+\t# Compute the moving average of all individual losses and the total loss.\n+\tloss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n+\tlosses = tf.get_collection(\'losses\')\n+\tloss_averages_op = loss_averages.apply(losses + [total_loss])\n   \n-    # Attach a scalar summmary to all individual losses and the total loss; do the\n-    # same for the averaged version of the losses.\n-    for l in losses + [total_loss]:\n-        # Name each loss as \'(raw)\' and name the moving average version of the loss\n-        # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n+\t# Attach a scalar summmary to all individual losses and the total loss; do the\n+\t# same for the averaged version of the losses.\n+\tfor l in losses + [total_loss]:\n+\t\t# Name each loss as \'(raw)\' and name the moving average version of the loss\n+\t\t# as the original loss name.\n+\t\ttf.summary.scalar(l.op.name +\' (raw)\', l)\n+\t\ttf.summary.scalar(l.op.name, loss_averages.average(l))\n   \n-    return loss_averages_op\n+\treturn loss_averages_op\n \n def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n-    # Generate moving averages of all losses and associated summaries.\n-    loss_averages_op = _add_loss_summaries(total_loss)\n-\n-    # Compute gradients.\n-    with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n-        else:\n-            raise ValueError(\'Invalid optimization algorithm\')\n-    \n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n-    # Apply gradients.\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n+\t# Generate moving averages of all losses and associated summaries.\n+\tloss_averages_op = _add_loss_summaries(total_loss)\n+\n+\t# Compute gradients.\n+\twith tf.control_dependencies([loss_averages_op]):\n+\t\tif optimizer==\'ADAGRAD\':\n+\t\t\topt = tf.train.AdagradOptimizer(learning_rate)\n+\t\telif optimizer==\'ADADELTA\':\n+\t\t\topt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n+\t\telif optimizer==\'ADAM\':\n+\t\t\topt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n+\t\telif optimizer==\'RMSPROP\':\n+\t\t\topt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n+\t\telif optimizer==\'MOM\':\n+\t\t\topt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n+\t\telse:\n+\t\t\traise ValueError(\'Invalid optimization algorithm\')\n+\t\n+\t\tgrads = opt.compute_gradients(total_loss, update_gradient_vars)\n+\t\t\n+\t# Apply gradients.\n+\tapply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n   \n-    # Add histograms for trainable variables.\n-    if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n+\t# Add histograms for trainable variables.\n+\tif log_histograms:\n+\t\tfor var in tf.trainable_variables():\n+\t\t\ttf.summary.histogram(var.op.name, var)\n    \n-    # Add histograms for gradients.\n-    if log_histograms:\n-        for grad, var in grads:\n-            if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n+\t# Add histograms for gradients.\n+\tif log_histograms:\n+\t\tfor grad, var in grads:\n+\t\t\tif grad is not None:\n+\t\t\t\ttf.summary.histogram(var.op.name + \'/gradients\', grad)\n   \n-    # Track the moving averages of all trainable variables.\n-    variable_averages = tf.train.ExponentialMovingAverage(\n-        moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n+\t# Track the moving averages of all trainable variables.\n+\tvariable_averages = tf.train.ExponentialMovingAverage(\n+\t\tmoving_average_decay, global_step)\n+\tvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n   \n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n+\twith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n+\t\ttrain_op = tf.no_op(name=\'train\')\n   \n-    return train_op\n+\treturn train_op\n \n def prewhiten(x):\n-    mean = np.mean(x)\n-    std = np.std(x)\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n+\tmean = np.mean(x)\n+\tstd = np.std(x)\n+\tstd_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n+\ty = np.multiply(np.subtract(x, mean), 1/std_adj)\n+\treturn y  \n \n def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n-        sz1 = int(image.shape[1]//2)\n-        sz2 = int(image_size//2)\n-        if random_crop:\n-            diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n-        else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n-    return image\n+\tif image.shape[1]>image_size:\n+\t\tsz1 = int(image.shape[1]//2)\n+\t\tsz2 = int(image_size//2)\n+\t\tif random_crop:\n+\t\t\tdiff = sz1-sz2\n+\t\t\t(h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n+\t\telse:\n+\t\t\t(h, v) = (0,0)\n+\t\timage = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n+\treturn image\n   \n def flip(image, random_flip):\n-    if random_flip and np.random.choice([True, False]):\n-        image = np.fliplr(image)\n-    return image\n+\tif random_flip and np.random.choice([True, False]):\n+\t\timage = np.fliplr(image)\n+\treturn image\n \n def to_rgb(img):\n-    w, h = img.shape\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n-    return ret\n+\tw, h = img.shape\n+\tret = np.empty((w, h, 3), dtype=np.uint8)\n+\tret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n+\treturn ret\n   \n def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n-    nrof_samples = len(image_paths)\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\n-    for i in range(nrof_samples):\n-        img = misc.imread(image_paths[i])\n-        if img.ndim == 2:\n-            img = to_rgb(img)\n-        if do_prewhiten:\n-            img = prewhiten(img)\n-        img = crop(img, do_random_crop, image_size)\n-        img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n-    return images\n+\tnrof_samples = len(image_paths)\n+\timages = np.zeros((nrof_samples, image_size, image_size, 3))\n+\tfor i in range(nrof_samples):\n+\t\timg = misc.imread(image_paths[i])\n+\t\tif img.ndim == 2:\n+\t\t\timg = to_rgb(img)\n+\t\tif do_prewhiten:\n+\t\t\timg = prewhiten(img)\n+\t\timg = crop(img, do_random_crop, image_size)\n+\t\timg = flip(img, do_random_flip)\n+\t\timages[i,:,:,:] = img\n+\treturn images\n \n def get_label_batch(label_data, batch_size, batch_index):\n-    nrof_examples = np.size(label_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = label_data[j:j+batch_size]\n-    else:\n-        x1 = label_data[j:nrof_examples]\n-        x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n-    batch_int = batch.astype(np.int64)\n-    return batch_int\n+\tnrof_examples = np.size(label_data, 0)\n+\tj = batch_index*batch_size % nrof_examples\n+\tif j+batch_size<=nrof_examples:\n+\t\tbatch = label_data[j:j+batch_size]\n+\telse:\n+\t\tx1 = label_data[j:nrof_examples]\n+\t\tx2 = label_data[0:nrof_examples-j]\n+\t\tbatch = np.vstack([x1,x2])\n+\tbatch_int = batch.astype(np.int64)\n+\treturn batch_int\n \n def get_batch(image_data, batch_size, batch_index):\n-    nrof_examples = np.size(image_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n-    else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n-    batch_float = batch.astype(np.float32)\n-    return batch_float\n+\tnrof_examples = np.size(image_data, 0)\n+\tj = batch_index*batch_size % nrof_examples\n+\tif j+batch_size<=nrof_examples:\n+\t\tbatch = image_data[j:j+batch_size,:,:,:]\n+\telse:\n+\t\tx1 = image_data[j:nrof_examples,:,:,:]\n+\t\tx2 = image_data[0:nrof_examples-j,:,:,:]\n+\t\tbatch = np.vstack([x1,x2])\n+\tbatch_float = batch.astype(np.float32)\n+\treturn batch_float\n \n def get_triplet_batch(triplets, batch_index, batch_size):\n-    ax, px, nx = triplets\n-    a = get_batch(ax, int(batch_size/3), batch_index)\n-    p = get_batch(px, int(batch_size/3), batch_index)\n-    n = get_batch(nx, int(batch_size/3), batch_index)\n-    batch = np.vstack([a, p, n])\n-    return batch\n+\tax, px, nx = triplets\n+\ta = get_batch(ax, int(batch_size/3), batch_index)\n+\tp = get_batch(px, int(batch_size/3), batch_index)\n+\tn = get_batch(nx, int(batch_size/3), batch_index)\n+\tbatch = np.vstack([a, p, n])\n+\treturn batch\n \n def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                if par[1]==\'-\':\n-                    lr = -1\n-                else:\n-                    lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n+\twith open(filename, \'r\') as f:\n+\t\tfor line in f.readlines():\n+\t\t\tline = line.split(\'#\', 1)[0]\n+\t\t\tif line:\n+\t\t\t\tpar = line.strip().split(\':\')\n+\t\t\t\te = int(par[0])\n+\t\t\t\tif par[1]==\'-\':\n+\t\t\t\t\tlr = -1\n+\t\t\t\telse:\n+\t\t\t\t\tlr = float(par[1])\n+\t\t\t\tif e <= epoch:\n+\t\t\t\t\tlearning_rate = lr\n+\t\t\t\telse:\n+\t\t\t\t\treturn learning_rate\n \n class ImageClass():\n-    "Stores the paths to images for a given class"\n-    def __init__(self, name, image_paths):\n-        self.name = name\n-        self.image_paths = image_paths\n+\t"Stores the paths to images for a given class"\n+\tdef __init__(self, name, image_paths):\n+\t\tself.name = name\n+\t\tself.image_paths = image_paths\n   \n-    def __str__(self):\n-        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n+\tdef __str__(self):\n+\t\treturn self.name + \', \' + str(len(self.image_paths)) + \' images\'\n   \n-    def __len__(self):\n-        return len(self.image_paths)\n+\tdef __len__(self):\n+\t\treturn len(self.image_paths)\n   \n def get_dataset(path, has_class_directories=True):\n-    dataset = []\n-    path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n-    classes.sort()\n-    nrof_classes = len(classes)\n-    for i in range(nrof_classes):\n-        class_name = classes[i]\n-        facedir = os.path.join(path_exp, class_name)\n-        image_paths = get_image_paths(facedir)\n-        dataset.append(ImageClass(class_name, image_paths))\n-  \n-    return dataset\n+\t"""\xe8\xaf\xa5\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe7\x9a\x84\xe5\x85\xa5\xe5\x8f\xa3\n+\t\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x88\xe5\x8f\x82\xe8\x80\x83LFW\xef\xbc\x89\xe4\xbb\xa5\xe4\xba\xba\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x9c\x89\xe4\xb8\x80\xe5\xbc\xa0\xe6\x88\x96\xe5\xa4\x9a\xe5\xbc\xa0\xe4\xba\xba\xe8\x84\xb8\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n+\tCelebA\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\xba\xe8\x84\xb8\xe5\x9b\xbe\xe7\x89\x87\xe5\xaf\xb9\xe5\xba\x94\xe5\x93\xaa\xe4\xb8\xaa\xe4\xba\xba\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\n+\t"""\n+\tdataset = []\n+\tdataset_type = "CelebA"\n+\n+\tif dataset_type == "LFW":\n+\t\tpath_exp = os.path.expanduser(path)\n+\t\tclasses = [path for path in os.listdir(path_exp) \\\n+\t\t\t\t\t\tif os.path.isdir(os.path.join(path_exp, path))]\n+\t\tclasses.sort()\n+\t\tnrof_classes = len(classes)\n+\t\tfor i in range(nrof_classes):\n+\t\t\tclass_name = classes[i]\n+\t\t\tfacedir = os.path.join(path_exp, class_name)\n+\t\t\timage_paths = get_image_paths(facedir)\n+\t\t\tdataset.append(ImageClass(class_name, image_paths))\n+\telse: \n+\t\tmetafile = "/datadisk4/daniel/005_face/data/CelebA/identity_meta.txt"\t\t \n+\t\twith open(metafile,\'r\') as f1:\n+\t\t\ttxt = f1.readlines()\n+\t\tidentity_dict = {}\n+\t\tfor line in txt:\n+\t\t\tl = line.strip().split()\n+\t\t\timg_path = l[0]\n+\t\t\tperson = l[1]\n+\t\t\tif person not in identity_dict.keys():\n+\t\t\t\tidentity_dict[person] = [img_path]\n+\t\t\telse:\n+\t\t\t\tidentity_dict[person].append(img_path)\n+\n+\t\tfor class_name, image_paths in identity_dict.items():\n+\t\t\tdataset.append(ImageClass(class_name, image_paths))\n+\n+\treturn dataset\n \n def get_image_paths(facedir):\n-    image_paths = []\n-    if os.path.isdir(facedir):\n-        images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n-    return image_paths\n+\timage_paths = []\n+\tif os.path.isdir(facedir):\n+\t\timages = os.listdir(facedir)\n+\t\timage_paths = [os.path.join(facedir,img) for img in images]\n+\treturn image_paths\n   \n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n-        nrof_classes = len(dataset)\n-        class_indices = np.arange(nrof_classes)\n-        np.random.shuffle(class_indices)\n-        split = int(round(nrof_classes*(1-split_ratio)))\n-        train_set = [dataset[i] for i in class_indices[0:split]]\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n-        train_set = []\n-        test_set = []\n-        for cls in dataset:\n-            paths = cls.image_paths\n-            np.random.shuffle(paths)\n-            nrof_images_in_class = len(paths)\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n-                split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n-                train_set.append(ImageClass(cls.name, paths[:split]))\n-                test_set.append(ImageClass(cls.name, paths[split:]))\n-    else:\n-        raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n-    return train_set, test_set\n+\tif mode==\'SPLIT_CLASSES\':\n+\t\tnrof_classes = len(dataset)\n+\t\tclass_indices = np.arange(nrof_classes)\n+\t\tnp.random.shuffle(class_indices)\n+\t\tsplit = int(round(nrof_classes*(1-split_ratio)))\n+\t\ttrain_set = [dataset[i] for i in class_indices[0:split]]\n+\t\ttest_set = [dataset[i] for i in class_indices[split:-1]]\n+\telif mode==\'SPLIT_IMAGES\':\n+\t\ttrain_set = []\n+\t\ttest_set = []\n+\t\tfor cls in dataset:\n+\t\t\tpaths = cls.image_paths\n+\t\t\tnp.random.shuffle(paths)\n+\t\t\tnrof_images_in_class = len(paths)\n+\t\t\tsplit = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n+\t\t\tif split==nrof_images_in_class:\n+\t\t\t\tsplit = nrof_images_in_class-1\n+\t\t\tif split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n+\t\t\t\ttrain_set.append(ImageClass(cls.name, paths[:split]))\n+\t\t\t\ttest_set.append(ImageClass(cls.name, paths[split:]))\n+\telse:\n+\t\traise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n+\treturn train_set, test_set\n \n def load_model(model, input_map=None):\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n-    #  or if it is a protobuf file with a frozen graph\n-    model_exp = os.path.expanduser(model)\n-    if (os.path.isfile(model_exp)):\n-        print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n-    else:\n-        print(\'Model directory: %s\' % model_exp)\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n-        print(\'Metagraph file: %s\' % meta_file)\n-        print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n+\t# Check if the model is a model directory (containing a metagraph and a checkpoint file)\n+\t#  or if it is a protobuf file with a frozen graph\n+\tmodel_exp = os.path.expanduser(model)\n+\tif (os.path.isfile(model_exp)):\n+\t\tprint(\'Model filename: %s\' % model_exp)\n+\t\twith gfile.FastGFile(model_exp,\'rb\') as f:\n+\t\t\tgraph_def = tf.GraphDef()\n+\t\t\tgraph_def.ParseFromString(f.read())\n+\t\t\ttf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n+\telse:\n+\t\tprint(\'Model directory: %s\' % model_exp)\n+\t\tmeta_file, ckpt_file = get_model_filenames(model_exp)\n+\t\t\n+\t\tprint(\'Metagraph file: %s\' % meta_file)\n+\t\tprint(\'Checkpoint file: %s\' % ckpt_file)\n+\t  \n+\t\tsaver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n+\t\tsaver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n+\t\n def get_model_filenames(model_dir):\n-    files = os.listdir(model_dir)\n-    meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n-    meta_file = meta_files[0]\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\n-    if ckpt and ckpt.model_checkpoint_path:\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n-        return meta_file, ckpt_file\n-\n-    meta_files = [s for s in files if \'.ckpt\' in s]\n-    max_step = -1\n-    for f in files:\n-        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n-            step = int(step_str.groups()[1])\n-            if step > max_step:\n-                max_step = step\n-                ckpt_file = step_str.groups()[0]\n-    return meta_file, ckpt_file\n+\tfiles = os.listdir(model_dir)\n+\tmeta_files = [s for s in files if s.endswith(\'.meta\')]\n+\tif len(meta_files)==0:\n+\t\traise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n+\telif len(meta_files)>1:\n+\t\traise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n+\tmeta_file = meta_files[0]\n+\tckpt = tf.train.get_checkpoint_state(model_dir)\n+\tif ckpt and ckpt.model_checkpoint_path:\n+\t\tckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n+\t\treturn meta_file, ckpt_file\n+\n+\tmeta_files = [s for s in files if \'.ckpt\' in s]\n+\tmax_step = -1\n+\tfor f in files:\n+\t\tstep_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n+\t\tif step_str is not None and len(step_str.groups())>=2:\n+\t\t\tstep = int(step_str.groups()[1])\n+\t\t\tif step > max_step:\n+\t\t\t\tmax_step = step\n+\t\t\t\tckpt_file = step_str.groups()[0]\n+\treturn meta_file, ckpt_file\n   \n def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n-        # Euclidian distance\n-        diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n-        # Distance based on cosine similarity\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n-        similarity = dot / norm\n-        dist = np.arccos(similarity) / math.pi\n-    else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n-    return dist\n+\tif distance_metric==0:\n+\t\t# Euclidian distance\n+\t\tdiff = np.subtract(embeddings1, embeddings2)\n+\t\tdist = np.sum(np.square(diff),1)\n+\telif distance_metric==1:\n+\t\t# Distance based on cosine similarity\n+\t\tdot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n+\t\tnorm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n+\t\tsimilarity = dot / norm\n+\t\tdist = np.arccos(similarity) / math.pi\n+\telse:\n+\t\traise \'Undefined distance metric %d\' % distance_metric \n+\t\t\n+\treturn dist\n \n def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n-    accuracy = np.zeros((nrof_folds))\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n-        # Find the best threshold for the fold\n-        acc_train = np.zeros((nrof_thresholds))\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n-        best_threshold_index = np.argmax(acc_train)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n-    return tpr, fpr, accuracy\n+\tassert(embeddings1.shape[0] == embeddings2.shape[0])\n+\tassert(embeddings1.shape[1] == embeddings2.shape[1])\n+\tnrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n+\tnrof_thresholds = len(thresholds)\n+\tk_fold = KFold(n_splits=nrof_folds, shuffle=False)\n+\t\n+\ttprs = np.zeros((nrof_folds,nrof_thresholds))\n+\tfprs = np.zeros((nrof_folds,nrof_thresholds))\n+\taccuracy = np.zeros((nrof_folds))\n+\t\n+\tindices = np.arange(nrof_pairs)\n+\t\n+\tfor fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n+\t\tif subtract_mean:\n+\t\t\tmean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+\t\telse:\n+\t\t  mean = 0.0\n+\t\tdist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n+\t\t\n+\t\t# Find the best threshold for the fold\n+\t\tacc_train = np.zeros((nrof_thresholds))\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\t_, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n+\t\tbest_threshold_index = np.argmax(acc_train)\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\ttprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n+\t\t_, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n+\t\t  \n+\t\ttpr = np.mean(tprs,0)\n+\t\tfpr = np.mean(fprs,0)\n+\treturn tpr, fpr, accuracy\n \n def calculate_accuracy(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n+\tpredict_issame = np.less(dist, threshold)\n+\ttp = np.sum(np.logical_and(predict_issame, actual_issame))\n+\tfp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n+\ttn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n+\tfn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n   \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n-    acc = float(tp+tn)/dist.size\n-    return tpr, fpr, acc\n+\ttpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n+\tfpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n+\tacc = float(tp+tn)/dist.size\n+\treturn tpr, fpr, acc\n \n \n   \n def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    val = np.zeros(nrof_folds)\n-    far = np.zeros(nrof_folds)\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n-        # Find the threshold that gives FAR = far_target\n-        far_train = np.zeros(nrof_thresholds)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n-            threshold = f(far_target)\n-        else:\n-            threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n+\tassert(embeddings1.shape[0] == embeddings2.shape[0])\n+\tassert(embeddings1.shape[1] == embeddings2.shape[1])\n+\tnrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n+\tnrof_thresholds = len(thresholds)\n+\tk_fold = KFold(n_splits=nrof_folds, shuffle=False)\n+\t\n+\tval = np.zeros(nrof_folds)\n+\tfar = np.zeros(nrof_folds)\n+\t\n+\tindices = np.arange(nrof_pairs)\n+\t\n+\tfor fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n+\t\tif subtract_mean:\n+\t\t\tmean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+\t\telse:\n+\t\t  mean = 0.0\n+\t\tdist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n+\t  \n+\t\t# Find the threshold that gives FAR = far_target\n+\t\tfar_train = np.zeros(nrof_thresholds)\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\t_, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n+\t\tif np.max(far_train)>=far_target:\n+\t\t\tf = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n+\t\t\tthreshold = f(far_target)\n+\t\telse:\n+\t\t\tthreshold = 0.0\n+\t\n+\t\tval[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n   \n-    val_mean = np.mean(val)\n-    far_mean = np.mean(far)\n-    val_std = np.std(val)\n-    return val_mean, val_std, far_mean\n+\tval_mean = np.mean(val)\n+\tfar_mean = np.mean(far)\n+\tval_std = np.std(val)\n+\treturn val_mean, val_std, far_mean\n \n \n def calculate_val_far(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    n_same = np.sum(actual_issame)\n-    n_diff = np.sum(np.logical_not(actual_issame))\n-    val = float(true_accept) / float(n_same)\n-    far = float(false_accept) / float(n_diff)\n-    return val, far\n+\tpredict_issame = np.less(dist, threshold)\n+\ttrue_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n+\tfalse_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n+\tn_same = np.sum(actual_issame)\n+\tn_diff = np.sum(np.logical_not(actual_issame))\n+\tval = float(true_accept) / float(n_same)\n+\tfar = float(false_accept) / float(n_diff)\n+\treturn val, far\n \n def store_revision_info(src_path, output_dir, arg_string):\n-    try:\n-        # Get git hash\n-        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_hash = stdout.strip()\n-    except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n+\ttry:\n+\t\t# Get git hash\n+\t\tcmd = [\'git\', \'rev-parse\', \'HEAD\']\n+\t\tgitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+\t\t(stdout, _) = gitproc.communicate()\n+\t\tgit_hash = stdout.strip()\n+\texcept OSError as e:\n+\t\tgit_hash = \' \'.join(cmd) + \': \' +  e.strerror\n   \n-    try:\n-        # Get local changes\n-        cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_diff = stdout.strip()\n-    except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n-    # Store a text file in the log directory\n-    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n-    with open(rev_info_filename, "w") as text_file:\n-        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n-        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n-        text_file.write(\'%s\' % git_diff)\n+\ttry:\n+\t\t# Get local changes\n+\t\tcmd = [\'git\', \'diff\', \'HEAD\']\n+\t\tgitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+\t\t(stdout, _) = gitproc.communicate()\n+\t\tgit_diff = stdout.strip()\n+\texcept OSError as e:\n+\t\tgit_diff = \' \'.join(cmd) + \': \' +  e.strerror\n+\t\n+\t# Store a text file in the log directory\n+\trev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n+\twith open(rev_info_filename, "w") as text_file:\n+\t\ttext_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n+\t\ttext_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n+\t\ttext_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n+\t\ttext_file.write(\'%s\' % git_diff)\n \n def list_variables(filename):\n-    reader = training.NewCheckpointReader(filename)\n-    variable_map = reader.get_variable_to_shape_map()\n-    names = sorted(variable_map.keys())\n-    return names\n+\treader = training.NewCheckpointReader(filename)\n+\tvariable_map = reader.get_variable_to_shape_map()\n+\tnames = sorted(variable_map.keys())\n+\treturn names\n \n def put_images_on_grid(images, shape=(16,8)):\n-    nrof_images = images.shape[0]\n-    img_size = images.shape[1]\n-    bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n-    for i in range(shape[1]):\n-        x_start = i*(img_size+bw)+bw\n-        for j in range(shape[0]):\n-            img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n-                break\n-            y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n-            break\n-    return img\n+\tnrof_images = images.shape[0]\n+\timg_size = images.shape[1]\n+\tbw = 3\n+\timg = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n+\tfor i in range(shape[1]):\n+\t\tx_start = i*(img_size+bw)+bw\n+\t\tfor j in range(shape[0]):\n+\t\t\timg_index = i*shape[0]+j\n+\t\t\tif img_index>=nrof_images:\n+\t\t\t\tbreak\n+\t\t\ty_start = j*(img_size+bw)+bw\n+\t\t\timg[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n+\t\tif img_index>=nrof_images:\n+\t\t\tbreak\n+\treturn img\n \n def write_arguments_to_file(args, filename):\n-    with open(filename, \'w\') as f:\n-        for key, value in iteritems(vars(args)):\n-            f.write(\'%s: %s\\n\' % (key, str(value)))\n+\twith open(filename, \'w\') as f:\n+\t\tfor key, value in iteritems(vars(args)):\n+\t\t\tf.write(\'%s: %s\\n\' % (key, str(value)))\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\nindex 475e81b..fc98992 100644\n--- a/src/models/inception_resnet_v1.py\n+++ b/src/models/inception_resnet_v1.py\n@@ -1,18 +1,3 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n """Contains the definition of the Inception Resnet V1 architecture.\n As described in http://arxiv.org/abs/1602.07261.\n   Inception-v4, Inception-ResNet and the Impact of Residual Connections\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\nindex c6ed4bb..ae8913d 100644\n--- a/src/train_tripletloss.py\n+++ b/src/train_tripletloss.py\n@@ -23,7 +23,6 @@ from tensorflow.python.ops import data_flow_ops\n from six.moves import xrange  # @UnresolvedImport\n \n def main(args):\n-  \n     network = importlib.import_module(args.model_def)\n \n     subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n@@ -148,7 +147,6 @@ def main(args):\n         tf.train.start_queue_runners(coord=coord, sess=sess)\n \n         with sess.as_default():\n-\n             if args.pretrained_model:\n                 print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n                 saver.restore(sess, os.path.expanduser(args.pretrained_model))\n@@ -396,9 +394,8 @@ def get_learning_rate_from_file(filename, epoch):\n \n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n     parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n+        help=\'Directory where to write event logs.\', default=\'./logs/\')\n     parser.add_argument(\'--models_base_dir\', type=str,\n         help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n     parser.add_argument(\'--gpu_memory_fraction\', type=float,'