arguments: train_tripletloss.py --checkpoint /datadisk2/daniel/models/facenet/20190327-151814/
--------------------
tensorflow version: 1.12.0
--------------------
git hash: b'db8cc08f0b59eb9696f8ad7e64197586f7e947d4'
--------------------
b'diff --git a/src/compare.py b/src/compare.py\nindex bc53cc4..67a3735 100644\n--- a/src/compare.py\n+++ b/src/compare.py\n@@ -1,48 +1,23 @@\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import os\n+os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n+os.environ["CUDA_VISIBLE_DEVICES"] = "1"\n from scipy import misc\n import tensorflow as tf\n import numpy as np\n import sys\n-import os\n import copy\n import argparse\n import facenet\n import align.detect_face\n \n def main(args):\n-\n     images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n     with tf.Graph().as_default():\n-\n         with tf.Session() as sess:\n-      \n             # Load the model\n             facenet.load_model(args.model)\n     \ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\nindex 4556bfa..1e2b497 100644\n--- a/src/decode_msceleb_dataset.py\n+++ b/src/decode_msceleb_dataset.py\n@@ -1,28 +1,3 @@\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\ndiff --git a/src/facenet.py b/src/facenet.py\nindex 0e05676..a2755c3 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -1,28 +1,4 @@\n-"""Functions for building the face recognition network.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-# pylint: disable=missing-docstring\n+#coding=utf-8\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n@@ -42,57 +18,57 @@ import math\n from six import iteritems\n \n def triplet_loss(anchor, positive, negative, alpha):\n-    """Calculate the triplet loss according to the FaceNet paper\n-    \n-    Args:\n-      anchor: the embeddings for the anchor images.\n-      positive: the embeddings for the positive images.\n-      negative: the embeddings for the negative images.\n+\t"""Calculate the triplet loss according to the FaceNet paper\n+\t\n+\tArgs:\n+\t  anchor: the embeddings for the anchor images.\n+\t  positive: the embeddings for the positive images.\n+\t  negative: the embeddings for the negative images.\n   \n-    Returns:\n-      the triplet loss according to the FaceNet paper as a float tensor.\n-    """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n-    return loss\n+\tReturns:\n+\t  the triplet loss according to the FaceNet paper as a float tensor.\n+\t"""\n+\twith tf.variable_scope(\'triplet_loss\'):\n+\t\tpos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n+\t\tneg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n+\t\t\n+\t\tbasic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n+\t\tloss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n+\t  \n+\treturn loss\n   \n def center_loss(features, label, alfa, nrof_classes):\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\n-    """\n-    nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n-    label = tf.reshape(label, [-1])\n-    centers_batch = tf.gather(centers, label)\n-    diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n-    with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n-    return loss, centers\n+\t"""Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n+\t   (http://ydwen.github.io/papers/WenECCV16.pdf)\n+\t"""\n+\tnrof_features = features.get_shape()[1]\n+\tcenters = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n+\t\tinitializer=tf.constant_initializer(0), trainable=False)\n+\tlabel = tf.reshape(label, [-1])\n+\tcenters_batch = tf.gather(centers, label)\n+\tdiff = (1 - alfa) * (centers_batch - features)\n+\tcenters = tf.scatter_sub(centers, label, diff)\n+\twith tf.control_dependencies([centers]):\n+\t\tloss = tf.reduce_mean(tf.square(features - centers_batch))\n+\treturn loss, centers\n \n def get_image_paths_and_labels(dataset):\n-    image_paths_flat = []\n-    labels_flat = []\n-    for i in range(len(dataset)):\n-        image_paths_flat += dataset[i].image_paths\n-        labels_flat += [i] * len(dataset[i].image_paths)\n-    return image_paths_flat, labels_flat\n+\timage_paths_flat = []\n+\tlabels_flat = []\n+\tfor i in range(len(dataset)):\n+\t\timage_paths_flat += dataset[i].image_paths\n+\t\tlabels_flat += [i] * len(dataset[i].image_paths)\n+\treturn image_paths_flat, labels_flat\n \n def shuffle_examples(image_paths, labels):\n-    shuffle_list = list(zip(image_paths, labels))\n-    random.shuffle(shuffle_list)\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n-    return image_paths_shuff, labels_shuff\n+\tshuffle_list = list(zip(image_paths, labels))\n+\trandom.shuffle(shuffle_list)\n+\timage_paths_shuff, labels_shuff = zip(*shuffle_list)\n+\treturn image_paths_shuff, labels_shuff\n \n def random_rotate_image(image):\n-    angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n+\tangle = np.random.uniform(low=-10.0, high=10.0)\n+\treturn misc.imrotate(image, angle, \'bicubic\')\n   \n # 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n RANDOM_ROTATE = 1\n@@ -101,471 +77,494 @@ RANDOM_FLIP = 4\n FIXED_STANDARDIZATION = 8\n FLIP = 16\n def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n-    images_and_labels_list = []\n-    for _ in range(nrof_preprocess_threads):\n-        filenames, label, control = input_queue.dequeue()\n-        images = []\n-        for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            #pylint: disable=no-member\n-            image.set_shape(image_size + (3,))\n-            images.append(image)\n-        images_and_labels_list.append([images, label])\n-\n-    image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n-        shapes=[image_size + (3,), ()], enqueue_many=True,\n-        capacity=4 * nrof_preprocess_threads * 100,\n-        allow_smaller_final_batch=True)\n-    \n-    return image_batch, label_batch\n+\timages_and_labels_list = []\n+\tfor _ in range(nrof_preprocess_threads):\n+\t\tfilenames, label, control = input_queue.dequeue()\n+\t\timages = []\n+\t\tfor filename in tf.unstack(filenames):\n+\t\t\tfile_contents = tf.read_file(filename)\n+\t\t\timage = tf.image.decode_image(file_contents, 3)\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n+\t\t\t\t\t\t\tlambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n+\t\t\t\t\t\t\tlambda:tf.random_crop(image, image_size + (3,)), \n+\t\t\t\t\t\t\tlambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n+\t\t\timage = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n+\t\t\t\t\t\t\tlambda:tf.image.random_flip_left_right(image),\n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n+\t\t\t\t\t\t\tlambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n+\t\t\t\t\t\t\tlambda:tf.image.per_image_standardization(image))\n+\t\t\timage = tf.cond(get_control_flag(control[0], FLIP),\n+\t\t\t\t\t\t\tlambda:tf.image.flip_left_right(image),\n+\t\t\t\t\t\t\tlambda:tf.identity(image))\n+\t\t\t#pylint: disable=no-member\n+\t\t\timage.set_shape(image_size + (3,))\n+\t\t\timages.append(image)\n+\t\timages_and_labels_list.append([images, label])\n+\n+\timage_batch, label_batch = tf.train.batch_join(\n+\t\timages_and_labels_list, batch_size=batch_size_placeholder, \n+\t\tshapes=[image_size + (3,), ()], enqueue_many=True,\n+\t\tcapacity=4 * nrof_preprocess_threads * 100,\n+\t\tallow_smaller_final_batch=True)\n+\t\n+\treturn image_batch, label_batch\n \n def get_control_flag(control, field):\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n+\treturn tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n   \n def _add_loss_summaries(total_loss):\n-    """Add summaries for losses.\n+\t"""Add summaries for losses.\n   \n-    Generates moving average for all losses and associated summaries for\n-    visualizing the performance of the network.\n+\tGenerates moving average for all losses and associated summaries for\n+\tvisualizing the performance of the network.\n   \n-    Args:\n-      total_loss: Total loss from loss().\n-    Returns:\n-      loss_averages_op: op for generating moving averages of losses.\n-    """\n-    # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\n+\tArgs:\n+\t  total_loss: Total loss from loss().\n+\tReturns:\n+\t  loss_averages_op: op for generating moving averages of losses.\n+\t"""\n+\t# Compute the moving average of all individual losses and the total loss.\n+\tloss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n+\tlosses = tf.get_collection(\'losses\')\n+\tloss_averages_op = loss_averages.apply(losses + [total_loss])\n   \n-    # Attach a scalar summmary to all individual losses and the total loss; do the\n-    # same for the averaged version of the losses.\n-    for l in losses + [total_loss]:\n-        # Name each loss as \'(raw)\' and name the moving average version of the loss\n-        # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n+\t# Attach a scalar summmary to all individual losses and the total loss; do the\n+\t# same for the averaged version of the losses.\n+\tfor l in losses + [total_loss]:\n+\t\t# Name each loss as \'(raw)\' and name the moving average version of the loss\n+\t\t# as the original loss name.\n+\t\ttf.summary.scalar(l.op.name +\' (raw)\', l)\n+\t\ttf.summary.scalar(l.op.name, loss_averages.average(l))\n   \n-    return loss_averages_op\n+\treturn loss_averages_op\n \n def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n-    # Generate moving averages of all losses and associated summaries.\n-    loss_averages_op = _add_loss_summaries(total_loss)\n-\n-    # Compute gradients.\n-    with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n-        else:\n-            raise ValueError(\'Invalid optimization algorithm\')\n-    \n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n-    # Apply gradients.\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n+\t# Generate moving averages of all losses and associated summaries.\n+\tloss_averages_op = _add_loss_summaries(total_loss)\n+\n+\t# Compute gradients.\n+\twith tf.control_dependencies([loss_averages_op]):\n+\t\tif optimizer==\'ADAGRAD\':\n+\t\t\topt = tf.train.AdagradOptimizer(learning_rate)\n+\t\telif optimizer==\'ADADELTA\':\n+\t\t\topt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n+\t\telif optimizer==\'ADAM\':\n+\t\t\topt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n+\t\telif optimizer==\'RMSPROP\':\n+\t\t\topt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n+\t\telif optimizer==\'MOM\':\n+\t\t\topt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n+\t\telse:\n+\t\t\traise ValueError(\'Invalid optimization algorithm\')\n+\t\n+\t\tgrads = opt.compute_gradients(total_loss, update_gradient_vars)\n+\t\t\n+\t# Apply gradients.\n+\tapply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n   \n-    # Add histograms for trainable variables.\n-    if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n+\t# Add histograms for trainable variables.\n+\tif log_histograms:\n+\t\tfor var in tf.trainable_variables():\n+\t\t\ttf.summary.histogram(var.op.name, var)\n    \n-    # Add histograms for gradients.\n-    if log_histograms:\n-        for grad, var in grads:\n-            if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n+\t# Add histograms for gradients.\n+\tif log_histograms:\n+\t\tfor grad, var in grads:\n+\t\t\tif grad is not None:\n+\t\t\t\ttf.summary.histogram(var.op.name + \'/gradients\', grad)\n   \n-    # Track the moving averages of all trainable variables.\n-    variable_averages = tf.train.ExponentialMovingAverage(\n-        moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n+\t# Track the moving averages of all trainable variables.\n+\tvariable_averages = tf.train.ExponentialMovingAverage(\n+\t\tmoving_average_decay, global_step)\n+\tvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n   \n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n+\twith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n+\t\ttrain_op = tf.no_op(name=\'train\')\n   \n-    return train_op\n+\treturn train_op\n \n def prewhiten(x):\n-    mean = np.mean(x)\n-    std = np.std(x)\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n+\tmean = np.mean(x)\n+\tstd = np.std(x)\n+\tstd_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n+\ty = np.multiply(np.subtract(x, mean), 1/std_adj)\n+\treturn y  \n \n def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n-        sz1 = int(image.shape[1]//2)\n-        sz2 = int(image_size//2)\n-        if random_crop:\n-            diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n-        else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n-    return image\n+\tif image.shape[1]>image_size:\n+\t\tsz1 = int(image.shape[1]//2)\n+\t\tsz2 = int(image_size//2)\n+\t\tif random_crop:\n+\t\t\tdiff = sz1-sz2\n+\t\t\t(h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n+\t\telse:\n+\t\t\t(h, v) = (0,0)\n+\t\timage = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n+\treturn image\n   \n def flip(image, random_flip):\n-    if random_flip and np.random.choice([True, False]):\n-        image = np.fliplr(image)\n-    return image\n+\tif random_flip and np.random.choice([True, False]):\n+\t\timage = np.fliplr(image)\n+\treturn image\n \n def to_rgb(img):\n-    w, h = img.shape\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n-    return ret\n+\tw, h = img.shape\n+\tret = np.empty((w, h, 3), dtype=np.uint8)\n+\tret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n+\treturn ret\n   \n def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n-    nrof_samples = len(image_paths)\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\n-    for i in range(nrof_samples):\n-        img = misc.imread(image_paths[i])\n-        if img.ndim == 2:\n-            img = to_rgb(img)\n-        if do_prewhiten:\n-            img = prewhiten(img)\n-        img = crop(img, do_random_crop, image_size)\n-        img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n-    return images\n+\tnrof_samples = len(image_paths)\n+\timages = np.zeros((nrof_samples, image_size, image_size, 3))\n+\tfor i in range(nrof_samples):\n+\t\timg = misc.imread(image_paths[i])\n+\t\tif img.ndim == 2:\n+\t\t\timg = to_rgb(img)\n+\t\tif do_prewhiten:\n+\t\t\timg = prewhiten(img)\n+\t\timg = crop(img, do_random_crop, image_size)\n+\t\timg = flip(img, do_random_flip)\n+\t\timages[i,:,:,:] = img\n+\treturn images\n \n def get_label_batch(label_data, batch_size, batch_index):\n-    nrof_examples = np.size(label_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = label_data[j:j+batch_size]\n-    else:\n-        x1 = label_data[j:nrof_examples]\n-        x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n-    batch_int = batch.astype(np.int64)\n-    return batch_int\n+\tnrof_examples = np.size(label_data, 0)\n+\tj = batch_index*batch_size % nrof_examples\n+\tif j+batch_size<=nrof_examples:\n+\t\tbatch = label_data[j:j+batch_size]\n+\telse:\n+\t\tx1 = label_data[j:nrof_examples]\n+\t\tx2 = label_data[0:nrof_examples-j]\n+\t\tbatch = np.vstack([x1,x2])\n+\tbatch_int = batch.astype(np.int64)\n+\treturn batch_int\n \n def get_batch(image_data, batch_size, batch_index):\n-    nrof_examples = np.size(image_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n-    else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n-    batch_float = batch.astype(np.float32)\n-    return batch_float\n+\tnrof_examples = np.size(image_data, 0)\n+\tj = batch_index*batch_size % nrof_examples\n+\tif j+batch_size<=nrof_examples:\n+\t\tbatch = image_data[j:j+batch_size,:,:,:]\n+\telse:\n+\t\tx1 = image_data[j:nrof_examples,:,:,:]\n+\t\tx2 = image_data[0:nrof_examples-j,:,:,:]\n+\t\tbatch = np.vstack([x1,x2])\n+\tbatch_float = batch.astype(np.float32)\n+\treturn batch_float\n \n def get_triplet_batch(triplets, batch_index, batch_size):\n-    ax, px, nx = triplets\n-    a = get_batch(ax, int(batch_size/3), batch_index)\n-    p = get_batch(px, int(batch_size/3), batch_index)\n-    n = get_batch(nx, int(batch_size/3), batch_index)\n-    batch = np.vstack([a, p, n])\n-    return batch\n+\tax, px, nx = triplets\n+\ta = get_batch(ax, int(batch_size/3), batch_index)\n+\tp = get_batch(px, int(batch_size/3), batch_index)\n+\tn = get_batch(nx, int(batch_size/3), batch_index)\n+\tbatch = np.vstack([a, p, n])\n+\treturn batch\n \n def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                if par[1]==\'-\':\n-                    lr = -1\n-                else:\n-                    lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n+\twith open(filename, \'r\') as f:\n+\t\tfor line in f.readlines():\n+\t\t\tline = line.split(\'#\', 1)[0]\n+\t\t\tif line:\n+\t\t\t\tpar = line.strip().split(\':\')\n+\t\t\t\te = int(par[0])\n+\t\t\t\tif par[1]==\'-\':\n+\t\t\t\t\tlr = -1\n+\t\t\t\telse:\n+\t\t\t\t\tlr = float(par[1])\n+\t\t\t\tif e <= epoch:\n+\t\t\t\t\tlearning_rate = lr\n+\t\t\t\telse:\n+\t\t\t\t\treturn learning_rate\n \n class ImageClass():\n-    "Stores the paths to images for a given class"\n-    def __init__(self, name, image_paths):\n-        self.name = name\n-        self.image_paths = image_paths\n+\t"Stores the paths to images for a given class"\n+\tdef __init__(self, name, image_paths):\n+\t\tself.name = name\n+\t\tself.image_paths = image_paths\n   \n-    def __str__(self):\n-        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n+\tdef __str__(self):\n+\t\treturn self.name + \', \' + str(len(self.image_paths)) + \' images\'\n   \n-    def __len__(self):\n-        return len(self.image_paths)\n+\tdef __len__(self):\n+\t\treturn len(self.image_paths)\n   \n def get_dataset(path, has_class_directories=True):\n-    dataset = []\n-    path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n-    classes.sort()\n-    nrof_classes = len(classes)\n-    for i in range(nrof_classes):\n-        class_name = classes[i]\n-        facedir = os.path.join(path_exp, class_name)\n-        image_paths = get_image_paths(facedir)\n-        dataset.append(ImageClass(class_name, image_paths))\n-  \n-    return dataset\n+\t"""\xe8\xaf\xa5\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe7\x9a\x84\xe5\x85\xa5\xe5\x8f\xa3\n+\t\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x88\xe5\x8f\x82\xe8\x80\x83LFW\xef\xbc\x89\xe4\xbb\xa5\xe4\xba\xba\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x9c\x89\xe4\xb8\x80\xe5\xbc\xa0\xe6\x88\x96\xe5\xa4\x9a\xe5\xbc\xa0\xe4\xba\xba\xe8\x84\xb8\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n+\tCelebA\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\xba\xe8\x84\xb8\xe5\x9b\xbe\xe7\x89\x87\xe5\xaf\xb9\xe5\xba\x94\xe5\x93\xaa\xe4\xb8\xaa\xe4\xba\xba\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\n+\t"""\n+\tdataset = []\n+\tdataset_type = "CelebA"\n+\n+\tif dataset_type == "LFW":\n+\t\tpath_exp = os.path.expanduser(path)\n+\t\tclasses = [path for path in os.listdir(path_exp) \\\n+\t\t\t\t\t\tif os.path.isdir(os.path.join(path_exp, path))]\n+\t\tclasses.sort()\n+\t\tnrof_classes = len(classes)\n+\t\tfor i in range(nrof_classes):\n+\t\t\tclass_name = classes[i]\n+\t\t\tfacedir = os.path.join(path_exp, class_name)\n+\t\t\timage_paths = get_image_paths(facedir)\n+\t\t\tdataset.append(ImageClass(class_name, image_paths))\n+\telse: \n+\t\tmetafile = "/datadisk4/daniel/005_face/data/CelebA/identity_meta.txt"\t\t \n+\t\twith open(metafile,\'r\') as f1:\n+\t\t\ttxt = f1.readlines()\n+\t\tidentity_dict = {}\n+\t\tfor line in txt:\n+\t\t\tl = line.strip().split()\n+\t\t\timg_path = l[0]\n+\t\t\tperson = l[1]\n+\t\t\tif person not in identity_dict.keys():\n+\t\t\t\tidentity_dict[person] = [img_path]\n+\t\t\telse:\n+\t\t\t\tidentity_dict[person].append(img_path)\n+\n+\t\tfor class_name, image_paths in identity_dict.items():\n+\t\t\tdataset.append(ImageClass(class_name, image_paths))\n+\n+\treturn dataset\n \n def get_image_paths(facedir):\n-    image_paths = []\n-    if os.path.isdir(facedir):\n-        images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n-    return image_paths\n+\timage_paths = []\n+\tif os.path.isdir(facedir):\n+\t\timages = os.listdir(facedir)\n+\t\timage_paths = [os.path.join(facedir,img) for img in images]\n+\treturn image_paths\n   \n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n-        nrof_classes = len(dataset)\n-        class_indices = np.arange(nrof_classes)\n-        np.random.shuffle(class_indices)\n-        split = int(round(nrof_classes*(1-split_ratio)))\n-        train_set = [dataset[i] for i in class_indices[0:split]]\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n-        train_set = []\n-        test_set = []\n-        for cls in dataset:\n-            paths = cls.image_paths\n-            np.random.shuffle(paths)\n-            nrof_images_in_class = len(paths)\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n-                split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n-                train_set.append(ImageClass(cls.name, paths[:split]))\n-                test_set.append(ImageClass(cls.name, paths[split:]))\n-    else:\n-        raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n-    return train_set, test_set\n+\tif mode==\'SPLIT_CLASSES\':\n+\t\tnrof_classes = len(dataset)\n+\t\tclass_indices = np.arange(nrof_classes)\n+\t\tnp.random.shuffle(class_indices)\n+\t\tsplit = int(round(nrof_classes*(1-split_ratio)))\n+\t\ttrain_set = [dataset[i] for i in class_indices[0:split]]\n+\t\ttest_set = [dataset[i] for i in class_indices[split:-1]]\n+\telif mode==\'SPLIT_IMAGES\':\n+\t\ttrain_set = []\n+\t\ttest_set = []\n+\t\tfor cls in dataset:\n+\t\t\tpaths = cls.image_paths\n+\t\t\tnp.random.shuffle(paths)\n+\t\t\tnrof_images_in_class = len(paths)\n+\t\t\tsplit = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n+\t\t\tif split==nrof_images_in_class:\n+\t\t\t\tsplit = nrof_images_in_class-1\n+\t\t\tif split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n+\t\t\t\ttrain_set.append(ImageClass(cls.name, paths[:split]))\n+\t\t\t\ttest_set.append(ImageClass(cls.name, paths[split:]))\n+\telse:\n+\t\traise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n+\treturn train_set, test_set\n \n def load_model(model, input_map=None):\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n-    #  or if it is a protobuf file with a frozen graph\n-    model_exp = os.path.expanduser(model)\n-    if (os.path.isfile(model_exp)):\n-        print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n-    else:\n-        print(\'Model directory: %s\' % model_exp)\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n-        print(\'Metagraph file: %s\' % meta_file)\n-        print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n+\t# Check if the model is a model directory (containing a metagraph and a checkpoint file)\n+\t#  or if it is a protobuf file with a frozen graph\n+\tmodel_exp = os.path.expanduser(model)\n+\tif (os.path.isfile(model_exp)):\n+\t\tprint(\'Model filename: %s\' % model_exp)\n+\t\twith gfile.FastGFile(model_exp,\'rb\') as f:\n+\t\t\tgraph_def = tf.GraphDef()\n+\t\t\tgraph_def.ParseFromString(f.read())\n+\t\t\ttf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n+\telse:\n+\t\tprint(\'Model directory: %s\' % model_exp)\n+\t\tmeta_file, ckpt_file = get_model_filenames(model_exp)\n+\t\t\n+\t\tprint(\'Metagraph file: %s\' % meta_file)\n+\t\tprint(\'Checkpoint file: %s\' % ckpt_file)\n+\t  \n+\t\tsaver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n+\t\tsaver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n+\t\n def get_model_filenames(model_dir):\n-    files = os.listdir(model_dir)\n-    meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n-    meta_file = meta_files[0]\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\n-    if ckpt and ckpt.model_checkpoint_path:\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n-        return meta_file, ckpt_file\n-\n-    meta_files = [s for s in files if \'.ckpt\' in s]\n-    max_step = -1\n-    for f in files:\n-        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n-            step = int(step_str.groups()[1])\n-            if step > max_step:\n-                max_step = step\n-                ckpt_file = step_str.groups()[0]\n-    return meta_file, ckpt_file\n+\tfiles = os.listdir(model_dir)\n+\tmeta_files = [s for s in files if s.endswith(\'.meta\')]\n+\tif len(meta_files)==0:\n+\t\traise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n+\telif len(meta_files)>1:\n+\t\traise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n+\tmeta_file = meta_files[0]\n+\tckpt = tf.train.get_checkpoint_state(model_dir)\n+\tif ckpt and ckpt.model_checkpoint_path:\n+\t\tckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n+\t\treturn meta_file, ckpt_file\n+\n+\tmeta_files = [s for s in files if \'.ckpt\' in s]\n+\tmax_step = -1\n+\tfor f in files:\n+\t\tstep_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n+\t\tif step_str is not None and len(step_str.groups())>=2:\n+\t\t\tstep = int(step_str.groups()[1])\n+\t\t\tif step > max_step:\n+\t\t\t\tmax_step = step\n+\t\t\t\tckpt_file = step_str.groups()[0]\n+\treturn meta_file, ckpt_file\n   \n def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n-        # Euclidian distance\n-        diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n-        # Distance based on cosine similarity\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n-        similarity = dot / norm\n-        dist = np.arccos(similarity) / math.pi\n-    else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n-    return dist\n+\tif distance_metric==0:\n+\t\t# Euclidian distance\n+\t\tdiff = np.subtract(embeddings1, embeddings2)\n+\t\tdist = np.sum(np.square(diff),1)\n+\telif distance_metric==1:\n+\t\t# Distance based on cosine similarity\n+\t\tdot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n+\t\tnorm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n+\t\tsimilarity = dot / norm\n+\t\tdist = np.arccos(similarity) / math.pi\n+\telse:\n+\t\traise \'Undefined distance metric %d\' % distance_metric \n+\t\t\n+\treturn dist\n \n def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n-    accuracy = np.zeros((nrof_folds))\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n-        # Find the best threshold for the fold\n-        acc_train = np.zeros((nrof_thresholds))\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n-        best_threshold_index = np.argmax(acc_train)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n-    return tpr, fpr, accuracy\n+\tassert(embeddings1.shape[0] == embeddings2.shape[0])\n+\tassert(embeddings1.shape[1] == embeddings2.shape[1])\n+\tnrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n+\tnrof_thresholds = len(thresholds)\n+\tk_fold = KFold(n_splits=nrof_folds, shuffle=False)\n+\t\n+\ttprs = np.zeros((nrof_folds,nrof_thresholds))\n+\tfprs = np.zeros((nrof_folds,nrof_thresholds))\n+\taccuracy = np.zeros((nrof_folds))\n+\t\n+\tindices = np.arange(nrof_pairs)\n+\t\n+\tfor fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n+\t\tif subtract_mean:\n+\t\t\tmean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+\t\telse:\n+\t\t  mean = 0.0\n+\t\tdist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n+\t\t\n+\t\t# Find the best threshold for the fold\n+\t\tacc_train = np.zeros((nrof_thresholds))\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\t_, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n+\t\tbest_threshold_index = np.argmax(acc_train)\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\ttprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n+\t\t_, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n+\t\t  \n+\t\ttpr = np.mean(tprs,0)\n+\t\tfpr = np.mean(fprs,0)\n+\treturn tpr, fpr, accuracy\n \n def calculate_accuracy(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n+\tpredict_issame = np.less(dist, threshold)\n+\ttp = np.sum(np.logical_and(predict_issame, actual_issame))\n+\tfp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n+\ttn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n+\tfn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n   \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n-    acc = float(tp+tn)/dist.size\n-    return tpr, fpr, acc\n+\ttpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n+\tfpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n+\tacc = float(tp+tn)/dist.size\n+\treturn tpr, fpr, acc\n \n \n   \n def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    val = np.zeros(nrof_folds)\n-    far = np.zeros(nrof_folds)\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n-        # Find the threshold that gives FAR = far_target\n-        far_train = np.zeros(nrof_thresholds)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n-            threshold = f(far_target)\n-        else:\n-            threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n+\tassert(embeddings1.shape[0] == embeddings2.shape[0])\n+\tassert(embeddings1.shape[1] == embeddings2.shape[1])\n+\tnrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n+\tnrof_thresholds = len(thresholds)\n+\tk_fold = KFold(n_splits=nrof_folds, shuffle=False)\n+\t\n+\tval = np.zeros(nrof_folds)\n+\tfar = np.zeros(nrof_folds)\n+\t\n+\tindices = np.arange(nrof_pairs)\n+\t\n+\tfor fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n+\t\tif subtract_mean:\n+\t\t\tmean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+\t\telse:\n+\t\t  mean = 0.0\n+\t\tdist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n+\t  \n+\t\t# Find the threshold that gives FAR = far_target\n+\t\tfar_train = np.zeros(nrof_thresholds)\n+\t\tfor threshold_idx, threshold in enumerate(thresholds):\n+\t\t\t_, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n+\t\tif np.max(far_train)>=far_target:\n+\t\t\tf = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n+\t\t\tthreshold = f(far_target)\n+\t\telse:\n+\t\t\tthreshold = 0.0\n+\t\n+\t\tval[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n   \n-    val_mean = np.mean(val)\n-    far_mean = np.mean(far)\n-    val_std = np.std(val)\n-    return val_mean, val_std, far_mean\n+\tval_mean = np.mean(val)\n+\tfar_mean = np.mean(far)\n+\tval_std = np.std(val)\n+\treturn val_mean, val_std, far_mean\n \n \n def calculate_val_far(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    n_same = np.sum(actual_issame)\n-    n_diff = np.sum(np.logical_not(actual_issame))\n-    val = float(true_accept) / float(n_same)\n-    far = float(false_accept) / float(n_diff)\n-    return val, far\n+\tpredict_issame = np.less(dist, threshold)\n+\ttrue_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n+\tfalse_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n+\tn_same = np.sum(actual_issame)\n+\tn_diff = np.sum(np.logical_not(actual_issame))\n+\tval = float(true_accept) / float(n_same)\n+\tfar = float(false_accept) / float(n_diff)\n+\treturn val, far\n \n def store_revision_info(src_path, output_dir, arg_string):\n-    try:\n-        # Get git hash\n-        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_hash = stdout.strip()\n-    except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n+\ttry:\n+\t\t# Get git hash\n+\t\tcmd = [\'git\', \'rev-parse\', \'HEAD\']\n+\t\tgitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+\t\t(stdout, _) = gitproc.communicate()\n+\t\tgit_hash = stdout.strip()\n+\texcept OSError as e:\n+\t\tgit_hash = \' \'.join(cmd) + \': \' +  e.strerror\n   \n-    try:\n-        # Get local changes\n-        cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_diff = stdout.strip()\n-    except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n-    # Store a text file in the log directory\n-    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n-    with open(rev_info_filename, "w") as text_file:\n-        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n-        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n-        text_file.write(\'%s\' % git_diff)\n+\ttry:\n+\t\t# Get local changes\n+\t\tcmd = [\'git\', \'diff\', \'HEAD\']\n+\t\tgitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+\t\t(stdout, _) = gitproc.communicate()\n+\t\tgit_diff = stdout.strip()\n+\texcept OSError as e:\n+\t\tgit_diff = \' \'.join(cmd) + \': \' +  e.strerror\n+\t\n+\t# Store a text file in the log directory\n+\trev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n+\twith open(rev_info_filename, "w") as text_file:\n+\t\ttext_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n+\t\ttext_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n+\t\ttext_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n+\t\ttext_file.write(\'%s\' % git_diff)\n \n def list_variables(filename):\n-    reader = training.NewCheckpointReader(filename)\n-    variable_map = reader.get_variable_to_shape_map()\n-    names = sorted(variable_map.keys())\n-    return names\n+\treader = training.NewCheckpointReader(filename)\n+\tvariable_map = reader.get_variable_to_shape_map()\n+\tnames = sorted(variable_map.keys())\n+\treturn names\n \n def put_images_on_grid(images, shape=(16,8)):\n-    nrof_images = images.shape[0]\n-    img_size = images.shape[1]\n-    bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n-    for i in range(shape[1]):\n-        x_start = i*(img_size+bw)+bw\n-        for j in range(shape[0]):\n-            img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n-                break\n-            y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n-            break\n-    return img\n+\tnrof_images = images.shape[0]\n+\timg_size = images.shape[1]\n+\tbw = 3\n+\timg = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n+\tfor i in range(shape[1]):\n+\t\tx_start = i*(img_size+bw)+bw\n+\t\tfor j in range(shape[0]):\n+\t\t\timg_index = i*shape[0]+j\n+\t\t\tif img_index>=nrof_images:\n+\t\t\t\tbreak\n+\t\t\ty_start = j*(img_size+bw)+bw\n+\t\t\timg[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n+\t\tif img_index>=nrof_images:\n+\t\t\tbreak\n+\treturn img\n \n def write_arguments_to_file(args, filename):\n-    with open(filename, \'w\') as f:\n-        for key, value in iteritems(vars(args)):\n-            f.write(\'%s: %s\\n\' % (key, str(value)))\n+\twith open(filename, \'w\') as f:\n+\t\tfor key, value in iteritems(vars(args)):\n+\t\t\tf.write(\'%s: %s\\n\' % (key, str(value)))\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\nindex 475e81b..fc98992 100644\n--- a/src/models/inception_resnet_v1.py\n+++ b/src/models/inception_resnet_v1.py\n@@ -1,18 +1,3 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n """Contains the definition of the Inception Resnet V1 architecture.\n As described in http://arxiv.org/abs/1602.07261.\n   Inception-v4, Inception-ResNet and the Impact of Residual Connections\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\nindex c6ed4bb..01d48f1 100644\n--- a/src/train_tripletloss.py\n+++ b/src/train_tripletloss.py\n@@ -23,443 +23,447 @@ from tensorflow.python.ops import data_flow_ops\n from six.moves import xrange  # @UnresolvedImport\n \n def main(args):\n-  \n-    network = importlib.import_module(args.model_def)\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n-        os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-\n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n-\n-    np.random.seed(seed=args.seed)\n-    train_set = facenet.get_dataset(args.data_dir)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n-    if args.pretrained_model:\n-        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n-    \n-    if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n-        # Read the file containing the pairs used for testing\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-        # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-        \n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-\n-        # Placeholder for the learning rate\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        \n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        \n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        \n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n-        \n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n-                                    dtypes=[tf.string, tf.int64],\n-                                    shapes=[(3,), (3,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n-        \n-        nrof_preprocess_threads = 4\n-        images_and_labels = []\n-        for _ in range(nrof_preprocess_threads):\n-            filenames, label = input_queue.dequeue()\n-            images = []\n-            for filename in tf.unstack(filenames):\n-                file_contents = tf.read_file(filename)\n-                image = tf.image.decode_image(file_contents, channels=3)\n-                \n-                if args.random_crop:\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n-                else:\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n-                if args.random_flip:\n-                    image = tf.image.random_flip_left_right(image)\n-    \n-                #pylint: disable=no-member\n-                image.set_shape((args.image_size, args.image_size, 3))\n-                images.append(tf.image.per_image_standardization(image))\n-            images_and_labels.append([images, label])\n-    \n-        image_batch, labels_batch = tf.train.batch_join(\n-            images_and_labels, batch_size=batch_size_placeholder, \n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        labels_batch = tf.identity(labels_batch, \'label_batch\')\n-\n-        # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n-            weight_decay=args.weight_decay)\n-        \n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n-        \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n-\n-        # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n-\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables())\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-\n-        # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n-\n-        # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n-\n-        # Initialize variables\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-\n-            if args.pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n-\n-            # Training and validation loop\n-            epoch = 0\n-            while epoch < args.max_nrof_epochs:\n-                step = sess.run(global_step, feed_dict=None)\n-                epoch = step // args.epoch_size\n-                # Train for one epoch\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\n-\n-                # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n-\n-                # Evaluate on LFW\n-                if args.lfw_dir:\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n-\n-    return model_dir\n+\tnetwork = importlib.import_module(args.model_def)\n+\n+\tsubdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n+\tlog_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n+\tif not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n+\t\tos.makedirs(log_dir)\n+\tmodel_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n+\tif not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n+\t\tos.makedirs(model_dir)\n+\n+\t# Write arguments to a text file\n+\tfacenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n+\t\t\n+\t# Store some git revision info in a text file in the log directory\n+\tsrc_path,_ = os.path.split(os.path.realpath(__file__))\n+\tfacenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n+\n+\tnp.random.seed(seed=args.seed)\n+\ttrain_set = facenet.get_dataset(args.data_dir)\n+\t\n+\tprint(\'Model directory: %s\' % model_dir)\n+\tprint(\'Log directory: %s\' % log_dir)\n+\tif args.pretrained_model:\n+\t\tprint(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n+\t\n+\tif args.lfw_dir:\n+\t\tprint(\'LFW directory: %s\' % args.lfw_dir)\n+\t\t# Read the file containing the pairs used for testing\n+\t\tpairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n+\t\t# Get the paths for the corresponding images\n+\t\tlfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n+\t\t\n+\t\n+\twith tf.Graph().as_default():\n+\t\ttf.set_random_seed(args.seed)\n+\t\tglobal_step = tf.Variable(0, trainable=False)\n+\n+\t\t# Placeholder for the learning rate\n+\t\tlearning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n+\t\t\n+\t\tbatch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n+\t\t\n+\t\tphase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n+\t\t\n+\t\timage_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n+\t\tlabels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n+\t\t\n+\t\tinput_queue = data_flow_ops.FIFOQueue(capacity=100000,\n+\t\t\t\t\t\t\t\t\tdtypes=[tf.string, tf.int64],\n+\t\t\t\t\t\t\t\t\tshapes=[(3,), (3,)],\n+\t\t\t\t\t\t\t\t\tshared_name=None, name=None)\n+\t\tenqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n+\t\t\n+\t\tnrof_preprocess_threads = 4\n+\t\timages_and_labels = []\n+\t\tfor _ in range(nrof_preprocess_threads):\n+\t\t\tfilenames, label = input_queue.dequeue()\n+\t\t\timages = []\n+\t\t\tfor filename in tf.unstack(filenames):\n+\t\t\t\tfile_contents = tf.read_file(filename)\n+\t\t\t\timage = tf.image.decode_image(file_contents, channels=3)\n+\t\t\t\t\n+\t\t\t\tif args.random_crop:\n+\t\t\t\t\timage = tf.random_crop(image, [args.image_size, args.image_size, 3])\n+\t\t\t\telse:\n+\t\t\t\t\timage = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n+\t\t\t\tif args.random_flip:\n+\t\t\t\t\timage = tf.image.random_flip_left_right(image)\n+\t\n+\t\t\t\t#pylint: disable=no-member\n+\t\t\t\timage.set_shape((args.image_size, args.image_size, 3))\n+\t\t\t\timages.append(tf.image.per_image_standardization(image))\n+\t\t\timages_and_labels.append([images, label])\n+\t\n+\t\timage_batch, labels_batch = tf.train.batch_join(\n+\t\t\timages_and_labels, batch_size=batch_size_placeholder, \n+\t\t\tshapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n+\t\t\tcapacity=4 * nrof_preprocess_threads * args.batch_size,\n+\t\t\tallow_smaller_final_batch=True)\n+\t\timage_batch = tf.identity(image_batch, \'image_batch\')\n+\t\timage_batch = tf.identity(image_batch, \'input\')\n+\t\tlabels_batch = tf.identity(labels_batch, \'label_batch\')\n+\n+\t\t# Build the inference graph\n+\t\tprelogits, _ = network.inference(image_batch, args.keep_probability, \n+\t\t\tphase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n+\t\t\tweight_decay=args.weight_decay)\n+\t\t\n+\t\tembeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n+\t\t# Split embeddings into anchor, positive and negative and calculate triplet loss\n+\t\tanchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n+\t\ttriplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n+\t\t\n+\t\tlearning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n+\t\t\targs.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n+\t\ttf.summary.scalar(\'learning_rate\', learning_rate)\n+\n+\t\t# Calculate the total losses\n+\t\tregularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n+\t\ttotal_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n+\n+\t\t# Build a Graph that trains the model with one batch of examples and updates the model parameters\n+\t\ttrain_op = facenet.train(total_loss, global_step, args.optimizer, \n+\t\t\tlearning_rate, args.moving_average_decay, tf.global_variables())\n+\t\t\n+\t\t# Create a saver\n+\t\tsaver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n+\n+\t\t# Build the summary operation based on the TF collection of Summaries.\n+\t\tsummary_op = tf.summary.merge_all()\n+\n+\t\t# Start running operations on the Graph.\n+\t\tgpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+\t\tsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\t\t\n+\n+\t\t# Initialize variables\n+\t\tsess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n+\t\tsess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n+\n+\t\tsummary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n+\t\tcoord = tf.train.Coordinator()\n+\t\ttf.train.start_queue_runners(coord=coord, sess=sess)\n+\n+\t\twith sess.as_default():\n+\t\t\tif args.pretrained_model:\n+\t\t\t\tprint(\'Restoring pretrained model: %s\' % args.pretrained_model)\n+\t\t\t\tsaver.restore(sess, os.path.expanduser(args.pretrained_model))\n+\t\t\tif args.checkpoint:\n+\t\t\t\tlatest_checkpoint = tf.train.latest_checkpoint(args.checkpoint)\n+\t\t\t\tsaver.restore(sess, latest_checkpoint)\n+\t\t\t\tprint("checkpoint restored from %s" % latest_checkpoint)\t\n+\n+\t\t\t# Training and validation loop\n+\t\t\tepoch = 0\n+\t\t\twhile epoch < args.max_nrof_epochs:\n+\t\t\t\tstep = sess.run(global_step, feed_dict=None)\n+\t\t\t\tepoch = step // args.epoch_size\n+\t\t\t\t# Train for one epoch\n+\t\t\t\ttrain(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n+\t\t\t\t\tbatch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n+\t\t\t\t\tembeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n+\t\t\t\t\targs.embedding_size, anchor, positive, negative, triplet_loss)\n+\n+\t\t\t\t# Save variables and the metagraph if it doesn\'t exist already\n+\t\t\t\tsave_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n+\n+\t\t\t\t# Evaluate on LFW\n+\t\t\t\tif args.lfw_dir:\n+\t\t\t\t\tevaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n+\t\t\t\t\t\t\tbatch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n+\t\t\t\t\t\t\targs.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n+\n+\treturn model_dir\n \n \n def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n-          embedding_size, anchor, positive, negative, triplet_loss):\n-    batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n-        lr = args.learning_rate\n-    else:\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-    while batch_number < args.epoch_size:\n-        # Sample people randomly from the dataset\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n-        \n-        print(\'Running forward pass on sampled images: \', end=\'\')\n-        start_time = time.time()\n-        nrof_examples = args.people_per_batch * args.images_per_person\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n-        for i in range(nrof_batches):\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\n-            emb_array[lab,:] = emb\n-        print(\'%.3f\' % (time.time()-start_time))\n-\n-        # Select triplets based on the embeddings\n-        print(\'Selecting suitable triplets for training\')\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n-            image_paths, args.people_per_batch, args.alpha)\n-        selection_time = time.time() - start_time\n-        print(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n-            (nrof_random_negs, nrof_triplets, selection_time))\n-\n-        # Perform training on the selected triplets\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n-        triplet_paths = list(itertools.chain(*triplets))\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n-        nrof_examples = len(triplet_paths)\n-        train_time = 0\n-        i = 0\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        loss_array = np.zeros((nrof_triplets,))\n-        summary = tf.Summary()\n-        step = 0\n-        while i < nrof_batches:\n-            start_time = time.time()\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n-            emb_array[lab,:] = emb\n-            loss_array[i] = err\n-            duration = time.time() - start_time\n-            print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\n-            batch_number += 1\n-            i += 1\n-            train_time += duration\n-            summary.value.add(tag=\'loss\', simple_value=err)\n-            \n-        # Add validation loss and accuracy to summary\n-        #pylint: disable=maybe-no-member\n-        summary.value.add(tag=\'time/selection\', simple_value=selection_time)\n-        summary_writer.add_summary(summary, step)\n-    return step\n+\t\t  batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n+\t\t  embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n+\t\t  embedding_size, anchor, positive, negative, triplet_loss):\n+\tbatch_number = 0\n+\t\n+\tif args.learning_rate>0.0:\n+\t\tlr = args.learning_rate\n+\telse:\n+\t\tlr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n+\twhile batch_number < args.epoch_size:\n+\t\t# Sample people randomly from the dataset\n+\t\timage_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n+\t\t\n+\t\tprint(\'Running forward pass on sampled images: \', end=\'\')\n+\t\tstart_time = time.time()\n+\t\tnrof_examples = args.people_per_batch * args.images_per_person\n+\t\tlabels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n+\t\timage_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n+\t\tsess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n+\t\temb_array = np.zeros((nrof_examples, embedding_size))\n+\t\tnrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n+\t\tfor i in range(nrof_batches):\n+\t\t\tbatch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n+\t\t\temb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n+\t\t\t\tlearning_rate_placeholder: lr, phase_train_placeholder: True})\n+\t\t\temb_array[lab,:] = emb\n+\t\tprint(\'%.3f\' % (time.time()-start_time))\n+\n+\t\t# Select triplets based on the embeddings\n+\t\tprint(\'Selecting suitable triplets for training\')\n+\t\ttriplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n+\t\t\timage_paths, args.people_per_batch, args.alpha)\n+\t\tselection_time = time.time() - start_time\n+\t\tprint(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n+\t\t\t(nrof_random_negs, nrof_triplets, selection_time))\n+\n+\t\t# Perform training on the selected triplets\n+\t\tnrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n+\t\ttriplet_paths = list(itertools.chain(*triplets))\n+\t\tlabels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n+\t\ttriplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n+\t\tsess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n+\t\tnrof_examples = len(triplet_paths)\n+\t\ttrain_time = 0\n+\t\ti = 0\n+\t\temb_array = np.zeros((nrof_examples, embedding_size))\n+\t\tloss_array = np.zeros((nrof_triplets,))\n+\t\tsummary = tf.Summary()\n+\t\tstep = 0\n+\t\twhile i < nrof_batches:\n+\t\t\tstart_time = time.time()\n+\t\t\tbatch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n+\t\t\tfeed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n+\t\t\terr, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n+\t\t\temb_array[lab,:] = emb\n+\t\t\tloss_array[i] = err\n+\t\t\tduration = time.time() - start_time\n+\t\t\tprint(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n+\t\t\t\t  (epoch, batch_number+1, args.epoch_size, duration, err))\n+\t\t\tbatch_number += 1\n+\t\t\ti += 1\n+\t\t\ttrain_time += duration\n+\t\t\tsummary.value.add(tag=\'loss\', simple_value=err)\n+\t\t\t\n+\t\t# Add validation loss and accuracy to summary\n+\t\t#pylint: disable=maybe-no-member\n+\t\tsummary.value.add(tag=\'time/selection\', simple_value=selection_time)\n+\t\tsummary_writer.add_summary(summary, step)\n+\treturn step\n   \n def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n-    """ Select the triplets for training\n-    """\n-    trip_idx = 0\n-    emb_start_idx = 0\n-    num_trips = 0\n-    triplets = []\n-    \n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n-    #  choosing the maximally violating example, as often done in structured output learning.\n-\n-    for i in xrange(people_per_batch):\n-        nrof_images = int(nrof_images_per_class[i])\n-        for j in xrange(1,nrof_images):\n-            a_idx = emb_start_idx + j - 1\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\n-                p_idx = emb_start_idx + pair\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n-                nrof_random_negs = all_neg.shape[0]\n-                if nrof_random_negs>0:\n-                    rnd_idx = np.random.randint(nrof_random_negs)\n-                    n_idx = all_neg[rnd_idx]\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n-                    #print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n-                    trip_idx += 1\n-\n-                num_trips += 1\n-\n-        emb_start_idx += nrof_images\n-\n-    np.random.shuffle(triplets)\n-    return triplets, num_trips, len(triplets)\n+\t""" Select the triplets for training\n+\t"""\n+\ttrip_idx = 0\n+\temb_start_idx = 0\n+\tnum_trips = 0\n+\ttriplets = []\n+\t\n+\t# VGG Face: Choosing good triplets is crucial and should strike a balance between\n+\t#  selecting informative (i.e. challenging) examples and swamping training with examples that\n+\t#  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n+\t#  the image n at random, but only between the ones that violate the triplet loss margin. The\n+\t#  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n+\t#  choosing the maximally violating example, as often done in structured output learning.\n+\n+\tfor i in xrange(people_per_batch):\n+\t\tnrof_images = int(nrof_images_per_class[i])\n+\t\tfor j in xrange(1,nrof_images):\n+\t\t\ta_idx = emb_start_idx + j - 1\n+\t\t\tneg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n+\t\t\tfor pair in xrange(j, nrof_images): # For every possible positive pair.\n+\t\t\t\tp_idx = emb_start_idx + pair\n+\t\t\t\tpos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n+\t\t\t\tneg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n+\t\t\t\t#all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n+\t\t\t\tall_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n+\t\t\t\tnrof_random_negs = all_neg.shape[0]\n+\t\t\t\tif nrof_random_negs>0:\n+\t\t\t\t\trnd_idx = np.random.randint(nrof_random_negs)\n+\t\t\t\t\tn_idx = all_neg[rnd_idx]\n+\t\t\t\t\ttriplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n+\t\t\t\t\t#print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n+\t\t\t\t\t#\t(trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n+\t\t\t\t\ttrip_idx += 1\n+\n+\t\t\t\tnum_trips += 1\n+\n+\t\temb_start_idx += nrof_images\n+\n+\tnp.random.shuffle(triplets)\n+\treturn triplets, num_trips, len(triplets)\n \n def sample_people(dataset, people_per_batch, images_per_person):\n-    nrof_images = people_per_batch * images_per_person\n+\tnrof_images = people_per_batch * images_per_person\n   \n-    # Sample classes from the dataset\n-    nrof_classes = len(dataset)\n-    class_indices = np.arange(nrof_classes)\n-    np.random.shuffle(class_indices)\n-    \n-    i = 0\n-    image_paths = []\n-    num_per_class = []\n-    sampled_class_indices = []\n-    # Sample images from these classes until we have enough\n-    while len(image_paths)<nrof_images:\n-        class_index = class_indices[i]\n-        nrof_images_in_class = len(dataset[class_index])\n-        image_indices = np.arange(nrof_images_in_class)\n-        np.random.shuffle(image_indices)\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n-        idx = image_indices[0:nrof_images_from_class]\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n-        sampled_class_indices += [class_index]*nrof_images_from_class\n-        image_paths += image_paths_for_class\n-        num_per_class.append(nrof_images_from_class)\n-        i+=1\n+\t# Sample classes from the dataset\n+\tnrof_classes = len(dataset)\n+\tclass_indices = np.arange(nrof_classes)\n+\tnp.random.shuffle(class_indices)\n+\t\n+\ti = 0\n+\timage_paths = []\n+\tnum_per_class = []\n+\tsampled_class_indices = []\n+\t# Sample images from these classes until we have enough\n+\twhile len(image_paths)<nrof_images:\n+\t\tclass_index = class_indices[i]\n+\t\tnrof_images_in_class = len(dataset[class_index])\n+\t\timage_indices = np.arange(nrof_images_in_class)\n+\t\tnp.random.shuffle(image_indices)\n+\t\tnrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n+\t\tidx = image_indices[0:nrof_images_from_class]\n+\t\timage_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n+\t\tsampled_class_indices += [class_index]*nrof_images_from_class\n+\t\timage_paths += image_paths_for_class\n+\t\tnum_per_class.append(nrof_images_from_class)\n+\t\ti+=1\n   \n-    return image_paths, num_per_class\n+\treturn image_paths, num_per_class\n \n def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\n-    start_time = time.time()\n-    # Run forward pass to calculate embeddings\n-    print(\'Running forward pass on LFW images: \', end=\'\')\n-    \n-    nrof_images = len(actual_issame)*2\n-    assert(len(image_paths)==nrof_images)\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\n-    label_check_array = np.zeros((nrof_images,))\n-    for i in xrange(nrof_batches):\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n-        emb_array[lab,:] = emb\n-        label_check_array[lab] = 1\n-    print(\'%.3f\' % (time.time()-start_time))\n-    \n-    assert(np.all(label_check_array==1))\n-    \n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n-    \n-    print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    lfw_time = time.time() - start_time\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n-    summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n+\t\tbatch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n+\t\tnrof_folds, log_dir, step, summary_writer, embedding_size):\n+\tstart_time = time.time()\n+\t# Run forward pass to calculate embeddings\n+\tprint(\'Running forward pass on LFW images: \', end=\'\')\n+\t\n+\tnrof_images = len(actual_issame)*2\n+\tassert(len(image_paths)==nrof_images)\n+\tlabels_array = np.reshape(np.arange(nrof_images),(-1,3))\n+\timage_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n+\tsess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n+\temb_array = np.zeros((nrof_images, embedding_size))\n+\tnrof_batches = int(np.ceil(nrof_images / batch_size))\n+\tlabel_check_array = np.zeros((nrof_images,))\n+\tfor i in xrange(nrof_batches):\n+\t\tbatch_size = min(nrof_images-i*batch_size, batch_size)\n+\t\temb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n+\t\t\tlearning_rate_placeholder: 0.0, phase_train_placeholder: False})\n+\t\temb_array[lab,:] = emb\n+\t\tlabel_check_array[lab] = 1\n+\tprint(\'%.3f\' % (time.time()-start_time))\n+\t\n+\tassert(np.all(label_check_array==1))\n+\t\n+\t_, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n+\t\n+\tprint(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n+\tprint(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n+\tlfw_time = time.time() - start_time\n+\t# Add validation loss and accuracy to summary\n+\tsummary = tf.Summary()\n+\t#pylint: disable=maybe-no-member\n+\tsummary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n+\tsummary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n+\tsummary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n+\tsummary_writer.add_summary(summary, step)\n+\twith open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n+\t\tf.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n \n def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n-    # Save the model checkpoint\n-    print(\'Saving variables\')\n-    start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-    save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n-    if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n-        start_time = time.time()\n-        saver.export_meta_graph(metagraph_filename)\n-        save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n-    summary_writer.add_summary(summary, step)\n+\t# Save the model checkpoint\n+\tprint(\'Saving variables\')\n+\tstart_time = time.time()\n+\tcheckpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n+\tsaver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n+\tsave_time_variables = time.time() - start_time\n+\tprint(\'Variables saved in %.2f seconds\' % save_time_variables)\n+\tmetagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n+\tsave_time_metagraph = 0  \n+\tif not os.path.exists(metagraph_filename):\n+\t\tprint(\'Saving metagraph\')\n+\t\tstart_time = time.time()\n+\t\tsaver.export_meta_graph(metagraph_filename)\n+\t\tsave_time_metagraph = time.time() - start_time\n+\t\tprint(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n+\tsummary = tf.Summary()\n+\t#pylint: disable=maybe-no-member\n+\tsummary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n+\tsummary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n+\tsummary_writer.add_summary(summary, step)\n   \n   \n def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n-    \n+\twith open(filename, \'r\') as f:\n+\t\tfor line in f.readlines():\n+\t\t\tline = line.split(\'#\', 1)[0]\n+\t\t\tif line:\n+\t\t\t\tpar = line.strip().split(\':\')\n+\t\t\t\te = int(par[0])\n+\t\t\t\tlr = float(par[1])\n+\t\t\t\tif e <= epoch:\n+\t\t\t\t\tlearning_rate = lr\n+\t\t\t\telse:\n+\t\t\t\t\treturn learning_rate\n+\t\n \n def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--people_per_batch\', type=int,\n-        help=\'Number of people per batch.\', default=45)\n-    parser.add_argument(\'--images_per_person\', type=int,\n-        help=\'Number of images per person.\', default=40)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--alpha\', type=float,\n-        help=\'Positive to negative triplet distance margin.\', default=0.2)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-\n-    # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    return parser.parse_args(argv)\n+\tparser = argparse.ArgumentParser()\n+\tparser.add_argument(\'--logs_base_dir\', type=str, \n+\t\thelp=\'Directory where to write event logs.\', default=\'./logs/\')\n+\tparser.add_argument(\'--models_base_dir\', type=str,\n+\t\thelp=\'Directory where to write trained models and checkpoints.\', default=\'./model_files/facenet\')\n+\tparser.add_argument(\'--gpu_memory_fraction\', type=float,\n+\t\thelp=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n+\tparser.add_argument(\'--pretrained_model\', type=str,\n+\t\thelp=\'Load a pretrained model before training starts.\')\n+\tparser.add_argument(\'--checkpoint\', type=str,\n+\t\thelp=\'continue from last checkpoint\')\n+\n+\tparser.add_argument(\'--data_dir\', type=str,\n+\t\thelp=\'Path to the data directory containing aligned face patches.\',\n+\t\tdefault=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n+\tparser.add_argument(\'--model_def\', type=str,\n+\t\thelp=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n+\tparser.add_argument(\'--max_nrof_epochs\', type=int,\n+\t\thelp=\'Number of epochs to run.\', default=500)\n+\tparser.add_argument(\'--batch_size\', type=int,\n+\t\thelp=\'Number of images to process in a batch.\', default=90)\n+\tparser.add_argument(\'--image_size\', type=int,\n+\t\thelp=\'Image size (height, width) in pixels.\', default=160)\n+\tparser.add_argument(\'--people_per_batch\', type=int,\n+\t\thelp=\'Number of people per batch.\', default=45)\n+\tparser.add_argument(\'--images_per_person\', type=int,\n+\t\thelp=\'Number of images per person.\', default=40)\n+\tparser.add_argument(\'--epoch_size\', type=int,\n+\t\thelp=\'Number of batches per epoch.\', default=1000)\n+\tparser.add_argument(\'--alpha\', type=float,\n+\t\thelp=\'Positive to negative triplet distance margin.\', default=0.2)\n+\tparser.add_argument(\'--embedding_size\', type=int,\n+\t\thelp=\'Dimensionality of the embedding.\', default=128)\n+\tparser.add_argument(\'--random_crop\', \n+\t\thelp=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n+\t\t \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n+\tparser.add_argument(\'--random_flip\', \n+\t\thelp=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n+\tparser.add_argument(\'--keep_probability\', type=float,\n+\t\thelp=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n+\tparser.add_argument(\'--weight_decay\', type=float,\n+\t\thelp=\'L2 weight regularization.\', default=0.0)\n+\tparser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n+\t\thelp=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n+\tparser.add_argument(\'--learning_rate\', type=float,\n+\t\thelp=\'Initial learning rate. If set to a negative value a learning rate \' +\n+\t\t\'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n+\tparser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n+\t\thelp=\'Number of epochs between learning rate decay.\', default=100)\n+\tparser.add_argument(\'--learning_rate_decay_factor\', type=float,\n+\t\thelp=\'Learning rate decay factor.\', default=1.0)\n+\tparser.add_argument(\'--moving_average_decay\', type=float,\n+\t\thelp=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n+\tparser.add_argument(\'--seed\', type=int,\n+\t\thelp=\'Random seed.\', default=666)\n+\tparser.add_argument(\'--learning_rate_schedule_file\', type=str,\n+\t\thelp=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n+\n+\t# Parameters for validation on LFW\n+\tparser.add_argument(\'--lfw_pairs\', type=str,\n+\t\thelp=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n+\tparser.add_argument(\'--lfw_dir\', type=str,\n+\t\thelp=\'Path to the data directory containing aligned face patches.\', default=\'\')\n+\tparser.add_argument(\'--lfw_nrof_folds\', type=int,\n+\t\thelp=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n+\treturn parser.parse_args(argv)\n   \n \n if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\n+\tmain(parse_arguments(sys.argv[1:]))'